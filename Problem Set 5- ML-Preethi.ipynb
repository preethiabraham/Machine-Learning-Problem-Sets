{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem Set 5 :\n",
    "# By Preethi Susan Abraham\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# import mglearn\n",
    "from sklearn import tree\n",
    "\n",
    "# Tree based models\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Kernel ridge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "# For timing: if you are interested \n",
    "from time import perf_counter\n",
    "\n",
    "# Usual cross-val tools\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Grid searching stuff\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17379 entries, 0 to 17378\n",
      "Data columns (total 17 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   instant     17379 non-null  int64  \n",
      " 1   dteday      17379 non-null  object \n",
      " 2   season      17379 non-null  int64  \n",
      " 3   yr          17379 non-null  int64  \n",
      " 4   mnth        17379 non-null  int64  \n",
      " 5   hr          17379 non-null  int64  \n",
      " 6   holiday     17379 non-null  int64  \n",
      " 7   weekday     17379 non-null  int64  \n",
      " 8   workingday  17379 non-null  int64  \n",
      " 9   weathersit  17379 non-null  int64  \n",
      " 10  temp        17379 non-null  float64\n",
      " 11  atemp       17379 non-null  float64\n",
      " 12  hum         17379 non-null  float64\n",
      " 13  windspeed   17379 non-null  float64\n",
      " 14  casual      17379 non-null  int64  \n",
      " 15  registered  17379 non-null  int64  \n",
      " 16  cnt         17379 non-null  int64  \n",
      "dtypes: float64(4), int64(12), object(1)\n",
      "memory usage: 2.3+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "instant       0\n",
       "dteday        0\n",
       "season        0\n",
       "yr            0\n",
       "mnth          0\n",
       "hr            0\n",
       "holiday       0\n",
       "weekday       0\n",
       "workingday    0\n",
       "weathersit    0\n",
       "temp          0\n",
       "atemp         0\n",
       "hum           0\n",
       "windspeed     0\n",
       "casual        0\n",
       "registered    0\n",
       "cnt           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bikedata = pd.read_csv(\"C:/Users/Preethi Abraham/Desktop/Brandeis Studies/Machine Learning and Data Analytics/bikeShareHour.csv\")\n",
    "# Checking for missing values (found none in any method)\n",
    "bikedata.info()\n",
    "bikedata.isnull()\n",
    "bikedata.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bike share data infoÂ¶\n",
    "- dteday : date\n",
    "- season : season (1:springer, 2:summer, 3:fall, 4:winter)\n",
    "- yr : year (0: 2011, 1:2012)\n",
    "- mnth : month ( 1 to 12)\n",
    "- hr : hour (0 to 23)\n",
    "- holiday : weather day is holiday or not (extracted from http://dchr.dc.gov/page/holiday-schedule)\n",
    "- weekday : day of the week\n",
    "- workingday : if day is neither weekend nor holiday is 1, otherwise is 0.\n",
    "+ weathersit : \n",
    "    - 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "    - 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "    - 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "    - 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n",
    "- temp : Normalized temperature in Celsius. The values are divided to 41 (max)\n",
    "- atemp: Normalized feeling temperature in Celsius. The values are divided to 50 (max)\n",
    "- hum: Normalized humidity. The values are divided to 100 (max)\n",
    "- windspeed: Normalized wind speed. The values are divided to 67 (max)\n",
    "- casual: count of casual users\n",
    "- registered: count of registered users\n",
    "- cnt: count of total rental bikes including both casual and registered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a peak hour dummy\n",
    "# This clever code converts the boolean to an integer by multiplying by 1\n",
    "bikedata['peakDummy'] = 1*(((bikedata.hr>=7) & (bikedata.hr<=9))|((bikedata.hr>=17)&(bikedata.hr<=19)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yr</th>\n",
       "      <th>hr</th>\n",
       "      <th>season</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>atemp</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>peakDummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   yr  hr  season  holiday  weekday  workingday  weathersit   atemp  \\\n",
       "0   0   0       1        0        6           0           1  0.2879   \n",
       "1   0   1       1        0        6           0           1  0.2727   \n",
       "2   0   2       1        0        6           0           1  0.2727   \n",
       "3   0   3       1        0        6           0           1  0.2879   \n",
       "4   0   4       1        0        6           0           1  0.2879   \n",
       "\n",
       "   windspeed  peakDummy  \n",
       "0        0.0          0  \n",
       "1        0.0          0  \n",
       "2        0.0          0  \n",
       "3        0.0          0  \n",
       "4        0.0          0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This creates data set of predictors that you should use\n",
    "X = bikedata.filter(['yr', 'hr', 'season', 'holiday', 'weekday', 'workingday', 'weathersit', 'atemp', 'windspeed', 'WeekendDummy', 'peakDummy'], axis=1)\n",
    "y = bikedata['cnt']\n",
    "# Reduce size of dataset\n",
    "X = X[:5000]\n",
    "y = y[:5000]\n",
    "print(X.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yr</th>\n",
       "      <th>hr</th>\n",
       "      <th>season</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>atemp</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>peakDummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.00000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>11.601400</td>\n",
       "      <td>1.836600</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>3.002800</td>\n",
       "      <td>0.680600</td>\n",
       "      <td>1.41840</td>\n",
       "      <td>0.459816</td>\n",
       "      <td>0.204327</td>\n",
       "      <td>0.252800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6.909824</td>\n",
       "      <td>0.729936</td>\n",
       "      <td>0.153064</td>\n",
       "      <td>2.019804</td>\n",
       "      <td>0.466291</td>\n",
       "      <td>0.63547</td>\n",
       "      <td>0.198876</td>\n",
       "      <td>0.123378</td>\n",
       "      <td>0.434661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.287900</td>\n",
       "      <td>0.104500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.469700</td>\n",
       "      <td>0.194000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>0.621200</td>\n",
       "      <td>0.283600</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.850700</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           yr           hr       season      holiday      weekday  \\\n",
       "count  5000.0  5000.000000  5000.000000  5000.000000  5000.000000   \n",
       "mean      0.0    11.601400     1.836600     0.024000     3.002800   \n",
       "std       0.0     6.909824     0.729936     0.153064     2.019804   \n",
       "min       0.0     0.000000     1.000000     0.000000     0.000000   \n",
       "25%       0.0     6.000000     1.000000     0.000000     1.000000   \n",
       "50%       0.0    12.000000     2.000000     0.000000     3.000000   \n",
       "75%       0.0    18.000000     2.000000     0.000000     5.000000   \n",
       "max       0.0    23.000000     3.000000     1.000000     6.000000   \n",
       "\n",
       "        workingday  weathersit        atemp    windspeed    peakDummy  \n",
       "count  5000.000000  5000.00000  5000.000000  5000.000000  5000.000000  \n",
       "mean      0.680600     1.41840     0.459816     0.204327     0.252800  \n",
       "std       0.466291     0.63547     0.198876     0.123378     0.434661  \n",
       "min       0.000000     1.00000     0.000000     0.000000     0.000000  \n",
       "25%       0.000000     1.00000     0.287900     0.104500     0.000000  \n",
       "50%       1.000000     1.00000     0.469700     0.194000     0.000000  \n",
       "75%       1.000000     2.00000     0.621200     0.283600     1.000000  \n",
       "max       1.000000     4.00000     1.000000     0.850700     1.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5476753680026929\n",
      "0.5396997924598971\n"
     ]
    }
   ],
   "source": [
    "# Linear regression\n",
    "# Linear regression\n",
    "nmc = 25\n",
    "shuffle = ShuffleSplit(n_splits=nmc, test_size=.2)\n",
    "lrm = LinearRegression()\n",
    "CVInfo = cross_validate(lrm, X, y, cv=shuffle,return_train_score=True,n_jobs=1)\n",
    "print(np.mean(CVInfo['train_score']))\n",
    "print(np.mean(CVInfo['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem you should use the bike sharing data set which is given below.  Also, use the starter notebook to clean and organize the data from the csv file.  This notebook also limits the data to the first 5000 instances (out of about 17,000).  This is done for speed and memory usage issues.  This data covers bike sharing activity in Washington DC.  Beyond cleaning, the starter file gives a simple linear regression too.\n",
    "\n",
    "For this problem we will look at the performance of three different models beyond linear regression.  In each case should should to a gridSearchCV, and use randomized cross validation to evaluate the different models. In each case use 25 randomized draws of training and test sets.  Set the test size to 0.2 of the sample.  All are performed on the X and y data array set up in the starter.  Model performance is judged by mean R-squared evaluated across the test samples.\n",
    "\n",
    "In each case also run a standard randomized cross validation test for 50 iterations at your optimal parameters from the search.\n",
    "\n",
    "Note:  All of these should run in less than 30 minutes.  Probably most will take under 15 minutes.  The slowest is the kernel ridge regression.  It also is the method that uses the most memory.  This is actually consistent with some timing results on kernel based methods which suggest they are not great for large sample sizes.\n",
    "\n",
    "1) Evaluate kernel ridge regression using alpha = [0.01,0.05,0.25, 0.5] and gamma = [0.01,0.05,0.25,0.5], and rbf kernel.  For this case make sure you apply the standard scaler to your data (use a pipeline).\n",
    "\n",
    "2) Evaluate a regression tree with max_depth equal to [5,10,15,20,25,50].\n",
    "\n",
    "3) Evaluate a random forest with max_depth equal to [5,10,15,20,25], max_features = [3,5,10], and n_estimators = [100].  (If you computer is very fast, you might try other numbers of trees of 10, and 25 just to check.  In solutions I will add a bunch more of these from my big machine.)\n",
    "\n",
    "4.) Which ended up being the best model, and best hyperparameter combination?\n",
    "\n",
    "5.) Why didn't you need to use the standardScaler in parts 2 and 3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    rank_test_score  mean_test_score param_kridge__alpha param_kridge__gamma\n",
      "0                13         0.796639                0.01                0.01\n",
      "1                 6         0.861405                0.01                0.05\n",
      "2                 5         0.864318                0.01                0.25\n",
      "3                10         0.841212                0.01                 0.5\n",
      "4                14         0.763394                0.05                0.01\n",
      "5                 9         0.849969                0.05                0.05\n",
      "6                 1         0.871705                0.05                0.25\n",
      "7                 4         0.865025                0.05                 0.5\n",
      "8                15         0.734064                0.25                0.01\n",
      "9                11         0.823873                0.25                0.05\n",
      "10                3         0.866076                0.25                0.25\n",
      "11                2         0.866284                0.25                 0.5\n",
      "12               16         0.717137                 0.5                0.01\n",
      "13               12         0.808547                 0.5                0.05\n",
      "14                7         0.860279                 0.5                0.25\n",
      "15                8         0.859880                 0.5                 0.5\n"
     ]
    }
   ],
   "source": [
    "#1.  Evaluate kernel ridge regression using alpha = [0.01,0.05,0.25, 0.5] and gamma = [0.01,0.05,0.25,0.5], and rbf kernel.  For this case make sure you apply the standard scaler to your data (use a pipeline).\n",
    "\n",
    "#For 25 splits\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "score_used = 'r2'\n",
    "fullModel = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"kridge\", KernelRidge())\n",
    "])\n",
    "\n",
    "param_grid={'kridge__alpha':[0.01,0.05,0.25, 0.5],'kridge__gamma':[0.01,0.05,0.25,0.5],'kridge__kernel':['rbf']}\n",
    "shuffle_split = ShuffleSplit(test_size=0.2, n_splits=25)\n",
    "grid_search=GridSearchCV(fullModel,param_grid,cv=shuffle_split,scoring=score_used, \n",
    "                              return_train_score=True,n_jobs=1)\n",
    "grid_search.fit(X,y)\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "print(results[['rank_test_score','mean_test_score','param_kridge__alpha','param_kridge__gamma']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    rank_test_score  mean_test_score param_kridge__alpha param_kridge__gamma\n",
      "0                13         0.794461                0.01                0.01\n",
      "1                 6         0.860253                0.01                0.05\n",
      "2                 4         0.862254                0.01                0.25\n",
      "3                10         0.833931                0.01                 0.5\n",
      "4                14         0.761181                0.05                0.01\n",
      "5                 9         0.848283                0.05                0.05\n",
      "6                 1         0.870591                0.05                0.25\n",
      "7                 5         0.861188                0.05                 0.5\n",
      "8                15         0.732065                0.25                0.01\n",
      "9                11         0.821896                0.25                0.05\n",
      "10                2         0.864298                0.25                0.25\n",
      "11                3         0.863040                0.25                 0.5\n",
      "12               16         0.715224                 0.5                0.01\n",
      "13               12         0.806505                 0.5                0.05\n",
      "14                7         0.858050                 0.5                0.25\n",
      "15                8         0.856245                 0.5                 0.5\n"
     ]
    }
   ],
   "source": [
    "#1. Evaluate kernel ridge regression using alpha = [0.01,0.05,0.25, 0.5] and gamma = [0.01,0.05,0.25,0.5], and rbf kernel.  For this case make sure you apply the standard scaler to your data (use a pipeline).\n",
    "\n",
    "\n",
    "#For 50 splits\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "score_used = 'r2'\n",
    "fullModel = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"kridge\", KernelRidge())\n",
    "])\n",
    "\n",
    "param_grid={'kridge__alpha':[0.01,0.05,0.25, 0.5],'kridge__gamma':[0.01,0.05,0.25,0.5],'kridge__kernel':['rbf']}\n",
    "shuffle_split = ShuffleSplit(test_size=0.2, n_splits=50)\n",
    "grid_search=GridSearchCV(fullModel,param_grid,cv=shuffle_split,scoring=score_used, \n",
    "                              return_train_score=True,n_jobs=1)\n",
    "grid_search.fit(X,y)\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "print(results[['rank_test_score','mean_test_score','param_kridge__alpha','param_kridge__gamma']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5136933829244331\n",
      "-0.5189331342799605\n"
     ]
    }
   ],
   "source": [
    "# standard randomized cross validation test\n",
    "CVInfo = cross_validate(fullModel, X, y, cv=shuffle,return_train_score=True,n_jobs=1)\n",
    "print(np.mean(CVInfo['train_score']))\n",
    "print(np.mean(CVInfo['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rank_test_score param_max_depth  mean_test_score\n",
      "0                6               5         0.792745\n",
      "1                1              10         0.896364\n",
      "2                2              15         0.878976\n",
      "3                4              20         0.875133\n",
      "4                5              25         0.874288\n",
      "5                3              50         0.875319\n"
     ]
    }
   ],
   "source": [
    "#2.Evaluate a regression tree with max_depth equal to [5,10,15,20,25,50].\n",
    "\n",
    "#For 25 splits\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "score_used = 'r2' \n",
    "param_grid={'max_depth':[5,10,15,20,25,50]}\n",
    "shuffle_split = ShuffleSplit(test_size=0.2, n_splits=25)\n",
    "grid_search=GridSearchCV(DecisionTreeRegressor(),param_grid,cv=shuffle_split,scoring=score_used, \n",
    "                              return_train_score=True)\n",
    "grid_search.fit(X,y)\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "print(results[['rank_test_score','param_max_depth','mean_test_score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ShuffleSplit(n_splits=50, random_state=None, test_size=0.2, train_size=None),\n",
       "             error_score=nan,\n",
       "             estimator=DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse',\n",
       "                                             max_depth=None, max_features=None,\n",
       "                                             max_leaf_nodes=None,\n",
       "                                             min_impurity_decrease=0.0,\n",
       "                                             min_impurity_split=None,\n",
       "                                             min_samples_leaf=1,\n",
       "                                             min_samples_split=2,\n",
       "                                             min_weight_fraction_leaf=0.0,\n",
       "                                             presort='deprecated',\n",
       "                                             random_state=None,\n",
       "                                             splitter='best'),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'max_depth': [5, 10, 15, 20, 25, 50]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "             scoring='r2', verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For 50 splits\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "score_used = 'r2' \n",
    "param_grid={'max_depth':[5,10,15,20,25,50]}\n",
    "shuffle_split = ShuffleSplit(test_size=0.2, n_splits=50)\n",
    "grid_search=GridSearchCV(DecisionTreeRegressor(),param_grid,cv=shuffle_split,scoring=score_used, \n",
    "                              return_train_score=True)\n",
    "grid_search.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rank_test_score param_max_depth  mean_test_score\n",
      "0                6               5         0.794169\n",
      "1                1              10         0.897474\n",
      "2                2              15         0.883121\n",
      "3                4              20         0.877782\n",
      "4                3              25         0.878152\n",
      "5                5              50         0.877036\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "print(results[['rank_test_score','param_max_depth','mean_test_score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9563839036052764\n",
      "0.8996801929454334\n"
     ]
    }
   ],
   "source": [
    "# standard randomized cross validation test\n",
    "dtr=DecisionTreeRegressor(max_depth=10)\n",
    "CVInfo = cross_validate(dtr, X, y, cv=shuffle,return_train_score=True,n_jobs=1)\n",
    "print(np.mean(CVInfo['train_score']))\n",
    "print(np.mean(CVInfo['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    rank_test_score  mean_test_score param_max_depth param_n_estimators  \\\n",
      "0                15         0.746198               5                100   \n",
      "1                14         0.791701               5                100   \n",
      "2                13         0.807929               5                100   \n",
      "3                12         0.897967              10                100   \n",
      "4                 8         0.923336              10                100   \n",
      "5                 7         0.927492              10                100   \n",
      "6                11         0.922125              15                100   \n",
      "7                 2         0.933475              15                100   \n",
      "8                 4         0.930712              15                100   \n",
      "9                10         0.922694              20                100   \n",
      "10                3         0.933313              20                100   \n",
      "11                6         0.929852              20                100   \n",
      "12                9         0.922761              25                100   \n",
      "13                1         0.933510              25                100   \n",
      "14                5         0.930458              25                100   \n",
      "\n",
      "   param_max_features  \n",
      "0                   3  \n",
      "1                   5  \n",
      "2                  10  \n",
      "3                   3  \n",
      "4                   5  \n",
      "5                  10  \n",
      "6                   3  \n",
      "7                   5  \n",
      "8                  10  \n",
      "9                   3  \n",
      "10                  5  \n",
      "11                 10  \n",
      "12                  3  \n",
      "13                  5  \n",
      "14                 10  \n"
     ]
    }
   ],
   "source": [
    "#3.Evaluate a random forest with max_depth equal to [5,10,15,20,25], max_features = [3,5,10], and n_estimators = [100].  (If you computer is very fast, you might try other numbers of trees of 10, and 25 just to check.  In solutions I will add a bunch more of these from my big machine.)\n",
    "\n",
    "#For 25 splits\n",
    "core_used = 'r2' \n",
    "param_grid={'n_estimators':[100],'max_depth':[5,10,15,20,25],'max_features':[3,5,10]}\n",
    "shuffle_split = ShuffleSplit(test_size=0.2, n_splits=25)\n",
    "grid_search=GridSearchCV(RandomForestRegressor(),param_grid,cv=shuffle_split,scoring=score_used, \n",
    "                              return_train_score=True)\n",
    "grid_search.fit(X,y)\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "print(results[['rank_test_score','mean_test_score','param_max_depth','param_n_estimators','param_max_features']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    rank_test_score  mean_test_score param_max_depth param_n_estimators  \\\n",
      "0                15         0.747533               5                100   \n",
      "1                14         0.791692               5                100   \n",
      "2                13         0.809869               5                100   \n",
      "3                12         0.897781              10                100   \n",
      "4                 8         0.923555              10                100   \n",
      "5                 7         0.928781              10                100   \n",
      "6                11         0.922114              15                100   \n",
      "7                 2         0.934196              15                100   \n",
      "8                 4         0.931448              15                100   \n",
      "9                10         0.923061              20                100   \n",
      "10                3         0.933932              20                100   \n",
      "11                5         0.931296              20                100   \n",
      "12                9         0.923169              25                100   \n",
      "13                1         0.934228              25                100   \n",
      "14                6         0.931065              25                100   \n",
      "\n",
      "   param_max_features  \n",
      "0                   3  \n",
      "1                   5  \n",
      "2                  10  \n",
      "3                   3  \n",
      "4                   5  \n",
      "5                  10  \n",
      "6                   3  \n",
      "7                   5  \n",
      "8                  10  \n",
      "9                   3  \n",
      "10                  5  \n",
      "11                 10  \n",
      "12                  3  \n",
      "13                  5  \n",
      "14                 10  \n"
     ]
    }
   ],
   "source": [
    "#For 50 splits\n",
    "core_used = 'r2' \n",
    "param_grid={'n_estimators':[100],'max_depth':[5,10,15,20,25],'max_features':[3,5,10]}\n",
    "shuffle_split = ShuffleSplit(test_size=0.2, n_splits=50)\n",
    "grid_search=GridSearchCV(RandomForestRegressor(),param_grid,cv=shuffle_split,scoring=score_used, \n",
    "                              return_train_score=True)\n",
    "grid_search.fit(X,y)\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "print(results[['rank_test_score','mean_test_score','param_max_depth','param_n_estimators','param_max_features']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.987984739201689\n",
      "0.9325268122839403\n"
     ]
    }
   ],
   "source": [
    "# standard randomized cross validation test\n",
    "rfr=RandomForestRegressor(n_estimators=100,max_depth=15)\n",
    "CVInfo = cross_validate(rfr, X, y, cv=shuffle,return_train_score=True,n_jobs=1)\n",
    "print(np.mean(CVInfo['train_score']))\n",
    "print(np.mean(CVInfo['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Which ended up being the best model, and best hyperparameter combination?\n",
    "\n",
    "The RandomForestRegressor() model is the best model. The mean test score is the highest (0.934228) when hyperparameters are set to max_depth= 25, n_estimators=100 and max_features=5\n",
    "\n",
    "\n",
    "### 5. Why didn't you need to use the standardScaler in parts 2 and 3?\n",
    "\n",
    "The Regression Tree (Part 2) and Random Forest Regression (Part 3) algorithms do not need the features to be scaled. This is because these algorithms are not sensitive to the magnitude of the features. However, the features need to be scaled (standardized) when we use Kernel Ridge Regression (Part 1). This is because this algorithm will penalize the model based on the magnitude of the feature variables. If the variance of a feature is large, then the coefficients obtained from the kernel ridge regression will be small. As a result, the penalty will be low."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
